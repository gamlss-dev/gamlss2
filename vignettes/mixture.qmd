---
title: "Mixture Models"
format: 
  html:
    html-math-method: mathjax
    toc: true
    number-sections: true
bibliography: gamlss2.bib
nocite: |
  @Rigby+Stasinopoulos:2005
vignette: >
  %\VignetteIndexEntry{Mixture Models}
  %\VignetteEngine{quarto::html}
  %\VignetteDepends{gamlss2}
  %\VignetteKeywords{distributional regression, inference, forecasting}
  %\VignettePackage{gamlss2}
---

```{r preliminaries, echo=FALSE, message=FALSE, results="hide"}
library("gamlss2")
```

Mixture models are used for modeling data generated from multiple 
distinct processes, where each process can be described by a separate probability distribution.
The challenge is that for each observation, we do not know which process generated it,
and therefore we must infer the underlying latent components.

This vignette demonstrates how to fit a mixture of two normal distributions
where the mixing probabilities depend on a covariate.

### Model Overview

In a finite mixture model, each observation $y_i$, $i = 1, \ldots, n$,
is assumed to be generated from one of $K$ distinct underlying distributions.
The probability that an observation comes from the $k$-th component is determined by a 
mixing probability $\pi_k(x)$, which may depend on covariates. The model for
the probability density function (pdf) of $y_i$ is expressed as:

$$
f(y_i \mid \boldsymbol{x}_i) = \sum_{k=1}^{K} \pi_k(\boldsymbol{x}_i) f_k(y_i \mid \theta_k(\boldsymbol{x}_i))
$$

Where:

- $\pi_k(\boldsymbol{x}_i)$ is the probability that the $i$-th
  observation belongs to the $k$-th component.
- $f_k(y_i \mid \theta_k(\boldsymbol{x}_i))$ is the pdf of the $k$-th component,
  parameterized by $\theta_k(\boldsymbol{x}_i)$, which depends on covariates
  $\boldsymbol{x}_i$.
- The sum of $\pi_k(\boldsymbol{x}_i)$ equals 1.

### Two-Component Normal Mixture Model

For a two-component normal mixture model, the response $y_i$ comes from one
of two normal distributions, where the mixing probability $\pi(x)$ depends on
a covariate $x$. The pdf is given by:

$$
f(y_i \mid x_i) = \pi(x_i) \mathcal{N}(y_i \mid \mu_1(x_i), \sigma_1(x_i)) + (1 - \pi(x_i)) \mathcal{N}(y_i \mid \mu_2(x_i), \sigma_2(x_i))
$$

Where:

- $\pi(x_i) = \frac{1}{1 + \exp(-\eta(x_i))}$ is the mixing probability.
- $\mathcal{N}(y_i \mid \mu, \sigma)$ denotes a normal distribution with
  mean $\mu$ and standard deviation $\sigma$.
- The parameters $\mu_1(x_i), \sigma_1(x_i), \mu_2(x_i), \sigma_2(x_i)$ are
  functions of covariates $x_i$.

### GAMLSS Framework for the Mixture Model

In the GAMLSS framework, each parameter can be modeled as a function of covariates.
For the two-component normal mixture, we can use GAMLSS to specify models for:

1. Component Means $\mu_1(x), \mu_2(x)$:
   Modeled as smooth or linear functions of covariates $x$, e.g., $\mu_1(x) = s(x)$.
2. Component Standard Deviations $\sigma_1(x), \sigma_2(x)$:
   These vary with $x$, e.g., $\log(\sigma_1(x)) = s(x)$.
3. Mixing Probability $\pi(x)$: Modeled using a logistic regression,
   $\pi(x) = \frac{1}{1 + \exp(-\gamma_0 - \gamma_1 x)}$.

The full model becomes:

$$
y_i \mid x_i \sim \pi(x_i) \mathcal{N}(\mu_1(x_i), \sigma_1(x_i)) + (1 - \pi(x_i)) \mathcal{N}(\mu_2(x_i), \sigma_2(x_i))
$$

## Simulating Data

We begin by simulating data where the response `y` is generated by one of two
normal distributions, depending on a covariate `x`. The latent component membership is
determined by a logistic model, where the mixing probability is a function of `x`.

```{r}
#| fig-height: 5
#| fig-width: 6
#| fig-align: center
set.seed(123)

## simulate covariate
n <- 1000
x <- sort(runif(n, -pi, pi))

## logistic model for mixing probability (dependent on x)
## this creates a probability that varies with x using a logistic function
mix_prob <- 1 / (1 + exp(-1 - 0.5 * x))

## simulate latent component assignment based on covariate-dependent probability
z <- rbinom(n, 1, mix_prob)

## generate response based on the latent component
y <- ifelse(z == 1,
            sin(x) + rnorm(n, sd = 0.2),
            cos(x) + rnorm(n, sd = 0.2))

## combine into a data frame
d <- data.frame("x" = x, "y" = y)

## plot the data, color by latent component z
par(mar = c(4, 4, 4, 1))
plot(d, col = z + 1, main = "Simulated Data by Latent Component", 
     xlab = "x", ylab = "y")
```

## Defining a Custom Mixture Family

To fit the mixture model in _gamlss2_, we define a custom family of distributions.
In this case, we create a mixture of two normal distributions (using `dnorm()`)
where the mixing probabilities depend on a covariate.

```{r}
## mixture family definition for a normal mixture
NOmx <- function(...) {
  fam <- list(
    "family" = "Normal Mixture",
    "names" = c("mu1", "mu2", "sigma1", "sigma2", "pi"),
    "links" = c("mu1" = "identity", "mu2" = "identity",
                "sigma1" = "log", "sigma2" = "log", "pi" = "logit"),
    "d" = function(y, par, log = FALSE, ...) {
      d <- par$pi * dnorm(y, par$mu1, par$sigma1) +
        (1 - par$pi) * dnorm(y, par$mu2, par$sigma2)
      if(log) d <- log(d)
      return(d)
    }
  )
  class(fam) <- "gamlss2.family"
  return(fam)
}
```

Note that in this case, analytical derivatives for the likelihood function are not 
explicitly defined in the family. As a result, the parameter estimation relies on 
numerical derivatives. While this approach is feasible, defining analytical derivatives 
would significantly speed up the estimation process and improve computational efficiency.

## Fitting the Mixture Model

We now fit the mixture model. The model includes two smooth functions `s(x)` for the
means of the two components, and the mixing probability `pi` is modeled as a linear
function of `x`.

The model can be estimated with

```{r}
b <- gamlss2(y ~ s(x) | s(x) | 1 | 1 | x, data = d, family = NOmx)
```

Plot estimated effects.

```{r}
#| fig-height: 4
#| fig-width: 7
#| fig-align: center
par(mfrow = c(1, 2), mar = c(4, 4, 4, 1))
plot(b, model = "mu1", main = "mu1")
plot(b, model = "mu2", main = "mu2")
```

## Predicting and Visualizing Results 

We predict the component-specific means (`mu1`, `mu2`) and the mixing
probability (`pi`) from the fitted model. Then, we compare the predicted and true
component probabilities and plot the fitted means along with the data.

First, compute predicted parameters.

```{r}
p <- predict(b, type = "parameter")
```

Plot estimated vs. true component probabilities.

```{r}
#| fig-height: 5
#| fig-width: 6
#| fig-align: center
plot(p$pi, mix_prob, main = "Estimated vs. True Component Probabilities", 
     xlab = "Estimated Probability", ylab = "True Probability")
abline(0, 1)

## compute posterior component probabilities
d1 <- dnorm(d$y, p$mu1, p$sigma1)
d2 <- dnorm(d$y, p$mu2, p$sigma2)
totprob <- rowSums(cbind(p$pi * d1, (1 - p$pi) * d2))
c1 <- p$pi * d1  / totprob
c2 <- (1 - p$pi) * d2  / totprob

## get components
comp <- apply(cbind(c2, c1), 1, which.max)

## plot the data and fitted component means
par(mar = c(4, 4, 4, 1))
plot(y ~ x, data = d,
  main = "Fitted Component Means",
  xlab = "x", ylab = "y", col = comp)
lines(p$mu1 ~ x, col = 2, lwd = 4)
lines(p$mu2 ~ x, col = 1, lwd = 4)
```

## Summary

In this vignette, we have demonstrated how to fit a mixture model where the
mixing probabilities depend on a covariate. We simulated data, defined a
custom mixture family, and fitted the model to estimate the component means and
mixing probabilities. Finally, we compared the predicted component probabilities with
the true values and visualized the results.

