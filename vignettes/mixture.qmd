---
title: "Mixture Models"
format: 
  html:
    html-math-method: mathjax
    toc: true
    number-sections: true
bibliography: gamlss2.bib
nocite: |
  @Rigby+Stasinopoulos:2005
vignette: >
  %\VignetteIndexEntry{Mixture Models}
  %\VignetteEngine{quarto::html}
  %\VignetteDepends{gamlss2}
  %\VignetteKeywords{distributional regression, inference, forecasting}
  %\VignettePackage{gamlss2}
---

```{r preliminaries, echo=FALSE, message=FALSE, results="hide"}
library("gamlss2")
```

Mixture models are used for modeling data generated from multiple 
distinct processes, where each process can be described by a separate probability distribution.
The challenge is that for each observation, we do not know which process generated it,
and therefore we must infer the underlying latent classes (or components).

This vignette demonstrates how to fit a mixture of normal distributions where the mixing probabilities depend on a covariate.

## Simulating Data

We begin by simulating data where the response `y` is generated by one of two
normal distributions, depending on a covariate `x`. The latent class membership is
determined by a logistic model, where the mixing probability is a function of `x`.

```{r}
set.seed(123)

## simulate covariate
n <- 1000
x <- sort(runif(n, -pi, pi))

## logistic model for mixing probability (dependent on x)
## this creates a probability that varies with x using a logistic function
mix_prob <- 1 / (1 + exp(-1 - 0.5 * x))

## simulate latent component assignment based on covariate-dependent probability
z <- rbinom(n, 1, mix_prob)

## generate response based on the latent component
y <- ifelse(z == 1,
            sin(x) + rnorm(n, sd = 0.2),
            cos(x) + rnorm(n, sd = 0.2))

## combine into a data frame
d <- data.frame("x" = x, "y" = y)

## plot the data, color by latent component z
par(mar = c(4, 4, 4, 1))
plot(d, col = z + 1, main = "Simulated Data by Latent Class", 
     xlab = "x", ylab = "y")
```

## Defining a Custom Mixture Family

To fit the mixture model in _gamlss2_, we define a custom family of distributions.
In this case, we create a mixture of two normal distributions where the mixing
probabilities depend on a covariate.

```{r}
## mixture family definition for a normal mixture
NOmx <- function(...) {
  fam <- list(
    "family" = "Normal Mixture",
    "names" = c("mu1", "mu2", "sigma1", "sigma2", "pi"),
    "links" = c("mu1" = "identity", "mu2" = "identity",
                "sigma1" = "log", "sigma2" = "log", "pi" = "logit"),
    "d" = function(y, par, log = FALSE, ...) {
      d <- par$pi * dnorm(y, par$mu1, par$sigma1) +
        (1 - par$pi) * dnorm(y, par$mu2, par$sigma2)
      if(log) d <- log(d)
      return(d)
    }
  )
  class(fam) <- "gamlss2.family"
  return(fam)
}
```

## Fitting the Mixture Model

We now fit the mixture model. The model includes two smooth functions of `x` for the
means of the two components, and the mixing probability `pi` is modeled as a linear
function of `x`.

```{r}
## estimate the model
b <- gamlss2(y ~ s(x) | s(x) | 1 | 1 | x, data = d, family = NOmx)

## plot the estimated effects
par(mfrow = c(1, 2), mar = c(4, 4, 4, 1))
plot(b, model = "mu1", main = "mu1")
plot(b, model = "mu2", main = "mu2")
```

## Predicting and Visualizing Results

We predict the component-specific means (`mu1`, `mu2`) and the mixing
probability (`pi`) from the fitted model. Then, we compare the predicted and true
class probabilities and plot the fitted means along with the data.

```{r}
## predict the parameters
p <- predict(b, type = "parameter")

## compare estimated vs. true class probabilities
plot(p$pi, mix_prob, main = "Estimated vs. True Class Probabilities", 
     xlab = "Estimated Probability", ylab = "True Probability")
abline(0, 1)

## plot the data and fitted component means
par(mar = c(4, 4, 4, 1))
plot(y ~ x, data = d,
  main = "Fitted Component Means",
  xlab = "x", ylab = "y")
lines(p$mu1 ~ x, col = 2, lwd = 2)
lines(p$mu2 ~ x, col = 4, lwd = 2)
```

## Summary

In this vignette, we have demonstrated how to fit a mixture model where the
mixing probabilities depend on a covariate. We simulated data, defined a
custom mixture family, and fitted the model to estimate the component means and
mixing probabilities. Finally, we compared the predicted class probabilities with
the true values and visualized the results.

