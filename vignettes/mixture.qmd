---
title: "Mixture Models"
format: 
  html:
    html-math-method: mathjax
    toc: true
    number-sections: true
bibliography: gamlss2.bib
nocite: |
  @Rigby+Stasinopoulos:2005
vignette: >
  %\VignetteIndexEntry{Mixture Models}
  %\VignetteEngine{quarto::html}
  %\VignetteDepends{gamlss2}
  %\VignetteKeywords{distributional regression, inference, forecasting}
  %\VignettePackage{gamlss2}
---

```{r preliminaries, echo=FALSE, message=FALSE, results="hide"}
library("gamlss2")
```

Mixture models are used for modeling data generated from multiple 
distinct processes, where each process can be described by a separate probability distribution.
The challenge is that for each observation, we do not know which process generated it,
and therefore we must infer the underlying latent components.

This vignette demonstrates how to fit a flexible mixture of two normal distributions.

### Model Overview

In a finite mixture model, each observation $y_i$ for $i = 1, \ldots, n$, is assumed to be generated from one of $K$ distinct underlying distributions. The probability that an observation comes from the $k$-th component is determined by the mixing probability $\pi_k(\mathbf{x}_i)$, which may depend on covariates $\mathbf{x}_i$. The probability density function (pdf) of $y_i$ is expressed as

$$
f(y_i \mid \mathbf{x}_i) = \sum_{k=1}^{K} \pi_k(\mathbf{x}_i) f_k(y_i \mid \boldsymbol{\theta}_k(\mathbf{x}_i)).
$$

Where

- $\pi_k(\mathbf{x}_i)$ is the probability that the $i$-th observation belongs to the $k$-th component,
- $f_k(y_i \mid \boldsymbol{\theta}_k(\mathbf{x}_i))$ is the pdf of the $k$-th component, parameterized by $\boldsymbol{\theta}_k(\mathbf{x}_i) = (\theta_{1k}(\mathbf{x}_i), \ldots, \theta_{J_k}(\mathbf{x}_i))^\top$,
- and the sum of $\pi_k(\mathbf{x}_i)$ over all components equals 1.

### Two-Component Normal Mixture Model

For a two-component normal mixture model, the response $y_i$ is generated by one of two normal distributions. The pdf is given by

$$
f(y_i \mid \mathbf{x}_i) = \sum_{k=1}^2 \pi_k(\mathbf{x}_i) \mathcal{N}(y_i \mid \boldsymbol{\theta}_k = (\mu_k(\mathbf{x}_i), \sigma_k(\mathbf{x}_i))).
$$

Where

- $\mathcal{N}(y \mid \boldsymbol{\theta} = (\mu, \sigma))$ denotes a normal distribution with mean $\mu$ and standard deviation $\sigma$,
- the parameters $\boldsymbol{\theta}_k = (\mu_k(\mathbf{x}_i), \sigma_k(\mathbf{x}_i))^\top$ are functions of covariates $\mathbf{x}_i$,
- and to ensure the component probabilities sum to one, we set $\pi_2(\mathbf{x}_i) = 1 - \pi_1(\mathbf{x}_i)$.

### GAMLSS Framework for the Mixture Model

In the GAMLSS framework, each parameter $\theta_{jk}(\mathbf{x}_i)$ is modeled using GAM-type predictors, given by

$$
\eta_{jk}(\mathbf{x}_i) = f_1(\mathbf{x}_i) + \dots + f_{L_{jk}}(\mathbf{x}_i),
$$

which are linked to the parameters through $h_{jk}(\theta_{jk}(\mathbf{x}_i)) = \eta_{jk}(\mathbf{x}_i)$ using suitable link functions $h_{jk}(\cdot)$. The functions $f_l(\cdot)$ can be nonlinear smooth functions (typically estimated using regression splines), linear effects, or random effects, among others.

For the two-component normal mixture, we use

1. Identity link for the means: $\mu_k(\mathbf{x}_i) = \eta_{1k}(\mathbf{x}_i)$,
2. Log-link for the standard deviations: $\log(\sigma_k(\mathbf{x}_i)) = \eta_{2k}(\mathbf{x}_i)$,
3. Logit-link for the mixing probability: $\pi_k(\mathbf{x}_i) = \frac{1}{1 + \exp(-\eta_{\pi k}(\mathbf{x}_i))}$.

## Simulating Data

We begin by simulating data where the response `y` is generated by one of two
normal distributions, depending on a covariate `x`. The latent component membership is
determined by a logistic model, where the mixing probability is a function of `x`.

```{r}
#| fig-height: 5
#| fig-width: 6
#| fig-align: center
set.seed(123)

## simulate covariate
n <- 1000
x <- sort(runif(n, -pi, pi))

## logistic model for mixing probability (dependent on x)
## this creates a probability that varies with x using a logistic function
mix_prob <- 1 / (1 + exp(-1 - 0.5 * x))

## simulate latent component assignment based on covariate-dependent probability
z <- rbinom(n, 1, mix_prob)

## generate response based on the latent component
y <- ifelse(z == 1,
            sin(x) + rnorm(n, sd = 0.2),
            cos(x) + rnorm(n, sd = 0.2))

## combine into a data frame
d <- data.frame("x" = x, "y" = y)

## plot the data, color by latent component z
par(mar = c(4, 4, 4, 1))
plot(d, col = z + 1, main = "Simulated Data by Latent Component", 
     xlab = "x", ylab = "y")
```

## Defining a Custom Mixture Family

To fit the mixture model in _gamlss2_, we define a custom family of distributions.
In this case, we create a mixture of two normal distributions (using `dnorm()`)
where the mixing probabilities depend on a covariate.

```{r}
## mixture family definition for a normal mixture
NOmx <- function(...) {
  fam <- list(
    "family" = "Normal Mixture",
    "names" = c("mu1", "mu2", "sigma1", "sigma2", "pi"),
    "links" = c("mu1" = "identity", "mu2" = "identity",
                "sigma1" = "log", "sigma2" = "log", "pi" = "logit"),
    "d" = function(y, par, log = FALSE, ...) {
      d <- par$pi * dnorm(y, par$mu1, par$sigma1) +
        (1 - par$pi) * dnorm(y, par$mu2, par$sigma2)
      if(log) d <- log(d)
      return(d)
    }
  )
  class(fam) <- "gamlss2.family"
  return(fam)
}
```

Note that in this case, analytical derivatives for the likelihood function are not 
explicitly defined in the family. As a result, the parameter estimation relies on 
numerical derivatives. While this approach is feasible, defining analytical derivatives 
would significantly speed up the estimation process and improve computational efficiency.

## Fitting the Mixture Model

We now fit the mixture model. The model includes two smooth functions `s(x)` for the
means of the two components, and the mixing probability `pi` is modeled as a linear
function of `x`.

The model can be estimated with

```{r}
b <- gamlss2(y ~ s(x) | s(x) | 1 | 1 | x, data = d, family = NOmx)
```

Plot estimated effects.

```{r}
#| fig-height: 4
#| fig-width: 7
#| fig-align: center
par(mfrow = c(1, 2), mar = c(4, 4, 4, 1))
plot(b, model = "mu1", main = "mu1")
plot(b, model = "mu2", main = "mu2")
```

## Predicting and Visualizing Results 

We predict the component-specific means (`mu1`, `mu2`) and the mixing
probability (`pi`) from the fitted model. Then, we compare the predicted and true
component probabilities and plot the fitted means along with the data.

First, compute predicted parameters.

```{r}
p <- predict(b, type = "parameter")
```

Plot estimated vs. true component probabilities.

```{r}
#| fig-height: 5
#| fig-width: 6
#| fig-align: center
plot(p$pi, mix_prob, main = "Estimated vs. True Component Probabilities", 
     xlab = "Estimated Probability", ylab = "True Probability")
abline(0, 1)

## compute posterior component probabilities
d1 <- dnorm(d$y, p$mu1, p$sigma1)
d2 <- dnorm(d$y, p$mu2, p$sigma2)
totprob <- rowSums(cbind(p$pi * d1, (1 - p$pi) * d2))
c1 <- p$pi * d1  / totprob
c2 <- (1 - p$pi) * d2  / totprob

## get components
comp <- apply(cbind(c2, c1), 1, which.max)

## plot the data and fitted component means
par(mar = c(4, 4, 4, 1))
plot(y ~ x, data = d,
  main = "Fitted Component Means",
  xlab = "x", ylab = "y", col = comp)
lines(p$mu1 ~ x, col = 2, lwd = 4)
lines(p$mu2 ~ x, col = 1, lwd = 4)
```

## Summary

In this vignette, we have demonstrated how to fit a mixture model where the
mixing probabilities depend on a covariate. We simulated data, defined a
custom mixture family, and fitted the model to estimate the component means and
mixing probabilities. Finally, we compared the predicted component probabilities with
the true values and visualized the results.

